\documentclass{book}

% The following comment is interspersed throughout the document to help maintain
% a column limit of 80 characters while editing. It can be ignored.
% 0000000100000000020000000003000000000400000000050000000006000000000700000000008

\title{Parsing Foundations}
\newcommand{\booksubtitle}{A Study of Data and Parsing}
\newcommand{\booklicense}{Creative Commons Zero 1.0 Universal}

\author{Anonymous}
\newcommand{\authorsubtitle}{Seattle, WA}

% Create convenient commands \booktitle and \bookauthor
\makeatletter
\newcommand{\booktitle}{\@title}
\newcommand{\bookauthor}{\@author}
\makeatother

\newcommand{\newterm}[2]{\textit{#1}\index{#2}}

% This utf8 declaration is not needed for versions of latex > 2018 but may
% be helpful for older software. Eventually it may not be worth keeping.
\usepackage[utf8]{inputenc}  
\usepackage{hyperref} % \url tag and clickable links
\usepackage{fix-cm}   % this package allows large \fontsize
\usepackage{tikz}     % this is for graphics. e.g. rectangle on title page
\usepackage{amsmath}  % Used by equations

% The following dimensions specify 4.75" X 7.5" content on 6 3/8" by 9 1/4"
% paper. The paper width and height can be tweaked as required and the content
% should size to fit within the margins accordingly.
%
% The (inside) bindingoffset should be larger for books with more pages. Some
% standard recommended sizes are .375in minimum up to 1in for 600+ page books.
% Sizes .75in and .875in are also recommended roughly at 150 and 400 pages.
\usepackage[bindingoffset=0.625in,
            left=.5in, right=.5in,
            top=.8125in, bottom=.9375in,
            paperwidth=6.375in, paperheight=9.25in]{geometry}
% Here is an alternative geometry for reading on letter size paper:
% \usepackage[margin=.75in, paperwidth=8.5in, paperheight=11in]{geometry}

\renewcommand{\contentsname}{Table of Contents} % default is {Contents}
\usepackage{makeidx}
\makeindex % Initialize an index so we can add entries with \index

% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
%     Beginning of Document
%
\begin{document}
\frontmatter

% ---- Half Title Page ----
% current geometry will be restored after title page
\newgeometry{top=1.75in,bottom=.5in}
\begin{titlepage}
\begin{flushleft}

% Title of Book
\textbf{\fontfamily{qcs}\fontsize{48}{54}\selectfont Parsing\\Foundations\\}

% Draw a line 4pt high
\par\noindent\rule{\textwidth}{4pt}\\

% Subtitle of Book
% Shaded box from left to right. Text node is midway (centered).
\begin{tikzpicture}
\shade[bottom color=lightgray,top color=white]
    (0,0) rectangle (\textwidth, 1.5)
    node[midway] {\textbf{\large \textit{\booksubtitle}}};
\end{tikzpicture}

% Edition Number
\begin{flushright}
\Large Unfinished Draft of First Edition
\end{flushright}

\vspace{\fill}

\end{flushleft}
\end{titlepage}
\restoregeometry
% ---- End of Half Title Page ----


% No page numbers on the Frontispiece page
\thispagestyle{empty}


% ---- Title Page ----
% current geometry will be restored after title page
\newgeometry{top=1.75in,bottom=.5in}
\begin{titlepage}
\begin{flushleft}

% Title of Book
\textbf{\fontfamily{qcs}\fontsize{48}{54}\selectfont Parsing\\Foundations\\}

% Draw a line 4pt high
\par\noindent\rule{\textwidth}{4pt}\\

% Shaded box from left to right with Subtitle
% The text node is midway (centered).
\begin{tikzpicture}
\shade[bottom color=lightgray,top color=white]
    (0,0) rectangle (\textwidth, 1.5)
    node[midway] {\textbf{\large \textit{\booksubtitle}}};
\end{tikzpicture}

% Edition Number
\begin{flushright}
\Large Unfinished Draft of First Edition
\end{flushright}

\vspace{\fill}

% Author and Location
\textbf{\large \bookauthor}\\[3.5pt]
\textbf{\large \textit{\authorsubtitle}}

\vspace{\fill}

% Self Publishing Logo. Free to use: CC0 license.
% The source file is book.svg. If you change the svg, you must then convert
% it to pdf. There are many online and offline tools available to do that.
\begin{center}
\includegraphics{booksvg.pdf}\\[4pt]
\fontfamily{lmtt}\small{Self Publishers Worldwide\\
Seattle San Francisco New York\\
London Paris Rome Beijing Barcelona}
\end{center}

\end{flushleft}
\end{titlepage}
\restoregeometry
% ---- End of Title Page ----

% Do not show page numbers on colophon page
\thispagestyle{empty}

\begin{flushleft}
\vspace*{\fill}
This book was typeset using \LaTeX{} software.\\
\vspace{\fill}
Copyright \textcopyright{} \the\year{}  \bookauthor\\
License: \booklicense
\end{flushleft}

% A title page resets the page # to 1, but the second title page
% was actually page 3. So add two to page counter.
\addtocounter{page}{2}

% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
% CHAPTER: Preface
%

% The asterisk excludes chapter from the table of contents.
\chapter*{Preface}
Parsing is an important and well-established area within the field
of software development and nearly every programmer can benefit 
from a solid understanding of parsing. 

There is an enormous amount of information on the topic of parsing.
However, it requires a serious investment of time 
(and often times some money) to 
identify, locate, acquire, organize, and understand the books, papers, 
and online resources that will help you gain a solid 
understanding of the relevant issues. 
This book was written to help the reader navigate this large body of
information. There is also a significant 
amount of useful research which has been around
for several decades but has been hidden in obscurity, and so
a goal of this book is to present this research to a wider audience.

This book does not assume that the reader is an expert with set-builder 
notation\footnote{\url{https://en.wikipedia.org/wiki/Set-builder_notation}}
or category theory\footnote{\url{https://en.wikipedia.org/wiki/Category_theory}}
or that the reader is familiar with specific mathematical symbols
that have been used in previous papers on the subject.
Thorough explanations are preferred over terse explanations.

This document is licensed under the Creative
Commons Zero v1.0 Universal 
license\footnote{\url{https://creativecommons.org/publicdomain/zero/1.0/legalcode}}.
That means you are free to copy this content,
distribute it, include it in your own content, or sell it, and you may do so
\textit{without attribution}. Just copy what you want and move on.

\section*{Audience}
This book can serve as an introduction to topics on data and parsing for
the working software developer or student of computer science. The book
covers foundational topics of parsing using concrete examples and common
% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
tools used in the field. In an academic setting, the book would serve well as
a companion book for courses that only cover parsing as part of a larger
topic, such as compiling.

\section*{Prior Work}
Prior to the writing of this book there were no public domain 
books on parsing.

While there are quite a few papers and articles written for academic
journals, these articles are typically written for a narrow audience 
by use of vocabulary and notation.
Also, many important papers on parsing are not freely available; one must
purchase a copy in order to read it, or have access to a very good library.
Finally, while there are some good online resources such as the work of
Federico Tomassetti et al, they are lost within a sea of information that
is either outdated or poorly produced, or only covers a limited set of topics.

The following is a list of commercially published books that the author has been 
able to review. 

\begin{enumerate}
    \item Dick Grune, Ceriel J.H. Jacobs\\
    Parsing Techniques, A Practical Guide, Second Edition, 2008

    This is the best book focused on parsing for the general audience.

    \item Alfred V. Aho, Monica S. Lam, Ravi Sethi, Jeffrey D. Ullman\\
    Compilers: Principles, Techniques, and Tools, Second Edition, 2007

    This is a very popular book that covers lexing, parsing, and compilation.
    The chapters on lexing are particularly good, covering DFA construction.
    This book is commonly known as the "Dragon Book" due to the illustration on the cover.

    \item S. Sippu, E. Soisalon-Soininen\\
    Parsing Theory, 1988

    This book describes parsing in terms of category theory and is appropriate for
    students who have taken graduate level classes in mathematics.

    \item Nigel P. Chapman\\
    LR Parsing, Theory and Practice, 1987

    This book describes parsing in the language of mathematics and is appropriate
    for students who have taken undergraduate level classes in mathematics.
    
    \item John-Paul Tremblay, Paul G. Sorenson\\
    The Theory and Practice of Compiler Writing, 1985

    \item William A. Barrett, John D. Couch\\
    Compiler Construction: Theory and Practice, 1979
\end{enumerate}



\section*{Terms and Symbols}
Many of the ideas covered in this book originated in the 1950s and 
1960s, near the dawn of the computer age. In the decades since,
researchers and educators have defined quite a few names for various
concepts surrounding data and parsing. They have also adopted numerous
symbols and notations as a convenient shorthand to write down their ideas.
% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
While it can be tedious to learn (and explain) these names and symbols, it
is necessary if you want to be able to follow discussions or read
original research papers on these topics. In some cases, words that you may
be familiar with in common language may have a different definition in 
academic papers. This can be confusing if you are not already familiar with
that term. Therefore, in this book, we will error on the side of explaining
the academic terms and symbols when they are used. Sometimes it takes a 
number of definitions conspiring together to explain a larger concept, so
bear with us if it seems like we are just defining words for no apparent
reason.

\section*{Prerequisites}
A knowledge of programming concepts is required.

% Four-level Table of Contents
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\tableofcontents

\mainmatter



% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
% CHAPTER: Parsing Concepts
%
\chapter{Parsing Concepts}\label{chpar}

\section{Strings}

\section{Languages}

% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
\section{Grammars}
context-free grammars are an ideal.

\url{https://tratt.net/laurie/blog/entries/parsing_the_solved_problem_that_isnt.html}

\subsection{Terminals}
A terminal is an atomic element of a language.


\subsection{Nonterminals}
Nonterminals are often referred to as variables.

\subsection{Productions}
A production is a rule that map a nonterminal 
to a sequence of terminals.

\section{Tokens and their Sub-Grammars}
Terminals are typically referred to as tokens in practice. Technically, a
terminal is just a language element, while a token typically contains
additional information about the context in which the element was recognized.

Tokens are typically defined by their own grammar based on regular expressions.
This grammar is typically separate from the language grammar.

\url{http://savage.net.au/Ron/html/graphviz2.marpa/Lexing.and.Parsing.Overview.html#Grammars_and_Sub-grammars}

Modern parsing system are supporting Unicode with increasing frequency.
Unicode is its own subgrammar.

Therefore, it is likely that there will be three layers of grammar involved
in defining a language.

\section{Parse Trees}

\subsection{Concrete Syntax Tree}
A tree that accurately reflects the grammar that produced it.

\subsection{Abstract Syntax Tree}
This is a parse tree that is abstraction of a concrete (or grammar-based) parse tree
in order to be independent of the grammar that produced it.


\subsection{Document Parsing}
\begin{verbatim}
parse_tree = parse(input)
\end{verbatim}

\subsection{Stream Parsing}
\begin{verbatim}
for event in parse(input):
    handle_event(event)
\end{verbatim}

For bottom up parsing, events are produced as wholly formed subtrees are recognized.
For top down parsing, events are produced for creation of the root node, and 
for children of a previously parsed node.

\subsection{Incremental Parsing}
Parse trees are needed for incremental parsing:
\begin{verbatim}
new_parse_tree = reparse(old_parse_tree, text_delta)
\end{verbatim}
\url{https://channel9.msdn.com/Blogs/Seth-Juarez/Anders-Hejlsberg-on-Modern-Compiler-Construction}

\subsection{Language Servers}
\url{https://microsoft.github.io/language-server-protocol/}

\section{Parsing Applications}
There are important differences between parsing 
data files versus parsing computer languages.

\section{Parsing Software}
There are handwritten and generated parsers.

\chapter{Lexers}

\section{The Role of the Lexer}

Lexemes and Tokens

Why perform lexing independently from parsing?

Wirth, in Compiler Construction, states

"A partitioning of the compilation process into as many parts as possible was the predominant
technique until about 1980, because until then the available store was too small to
accommodate the entire compiler"\\



"The design decision to decouple the lexer and parser has existed since at least Antlr 2 and likely made for separation of concerns and performance reasons. Where the lexer is driven by the parser, the lexer may have to re-lex portions of the input text many times in different contexts before the parser is able to satisfy a rule. The Antlr lexer does everything it can, and largely succeeds, in processing the entire input text in a single pass. Just different approaches taken with the same good intentions."
\url{https://stackoverflow.com/questions/28873463/no-context-sensitivity-in-antlr4}\\


A pipeline supports separation of concerns and allows reuse of a lexer
with other parsers that accept a stream of tokens.

\url{https://rosettacode.org/wiki/Compiler/lexical_analyzer}\\
\url{https://rosettacode.org/wiki/Compiler/Sample_programs} 


\subsection{Lexers, Tokenizers, and Scanners}
The simplistic view is these are all the same.
The more nuanced view is that there are distinctions between
these words that can be helpful.

A \newterm{lexer}{Lexer} is software that takes data as input and outputs 
tokens, (i.e. it acts as a tokenizer) using a
scanner to process the input data.


\subsection{Scannerless Parsing}

The concept of \newterm{scannerless parsing}{Scannerless Parsing} was 
explored thoroughly by Salomon and Cormack in 
"Scannerless nslr(1) parsing of programming languages" July 1989
It may not be immediately clear why simply \textit{not} using a scanner
is an idea being discussed in the year 1989. If you read Knuth's 1965 paper on
LR parsing, there is no mention of scanning. One would presume that the lack
of a scanner was a normal thing. But strictly speaking, his 
paper was simply agnostic about the nature of the 
terminals. The parsing process
he describes presumes the input string is composed of a pure sequence 
of atomic elements. There is no further discussion on the possible
structure or origin of those elements.

In practice, however, a scanner is typically used because it
allows the resulting grammar to be much simpler than a grammar
in which every character is a terminal. A simpler grammar can
often be parsed with a simpler parser, which is an essential benefit
of using a scanner.

Explains the scanner is just an optimized parser:
\url{https://softwareengineering.stackexchange.com/questions/337676/what-is-the-procedure-that-is-followed-when-writing-a-lexer-based-upon-a-grammar}

Explains that scannerless parsing doesn't have all the cool regex features like negation:
\url{https://github.com/antlr/antlr4/issues/1527} Please, remove lexer from Antlr

\section{Scanning}

\subsection{Input Data}

\subsection{Rules for Tokens}
                      
\subsection{Literals}

\subsection{Regular Expressions}

\url{https://regex101.com/}
\url{https://news.ycombinator.com/item?id=23975041}

Using capture groups for acceptance allows non-captured expressions to indicate negation:
\url{http://www.rexegg.com/regex-best-trick.html}

\subsubsection{Regex DoS}

\url{https://levelup.gitconnected.com/the-regular-expression-denial-of-service-redos-cheat-sheet-a78d0ed7d865}
\url{https://blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/}

\subsection{Pseudo-Tokens}

\subsection{Context-Free and Context-Aware Scanning}
references: \url{https://www.umsec.umn.edu/publications/Context-Aware-Scanning-Parsing-Extensible}
\url{http://tree-sitter.github.io/tree-sitter/creating-parsers#lexical-analysis}


\section{Scanner Conflicts}

When two different token rules match the same string, that is a
\newterm{scanner conflict}{Scanner Conflict}.


\subsection{Traditional Lexer Precedence}
A \newterm{traditional lexer}{Traditional Lexer} resolves
conflicts between tokens using 
\newterm{traditional lexical precedence}{Traditional Lexical Precedence}
as defined by [Denny, Def 2.2.6] and repeated here.

Traditional lexers have been context-free lexers. They also
resolve conflicts using the 
\newterm{longest match}{Longest Match} rule first,
followed by \newterm{positional priority}{Positional Priority}
as described in the next two sections, respectively.

\subsubsection{Longest Match Rule}

\subsubsection{Positional Priority}

\subsection{Longest Match Conflict}
A longest match conflict occurs when a token chosen by
a traditional lexer leads to an error state in the lexer
or parser, whereas if a different token, such as a shorter
one was chosen, the parse would succeed.

For example, 1..10

If 1 and 1. are valid tokens (for integers and floating point numbers),
then these sequences are valid:
\begin{verbatim}
(1.) (.10)
(1) (..) (10)
\end{verbatim}

Longest match favors the first sequence, but that token sequence
is invalid to the parser, but the second sequence is valid.

The presence of this problem is a combination of:
\begin{itemize}
    \item
    The traditional lexical and grammatical model cannot express
this language without special handling.
    \item    Trying to use or design a language that requires this
special handling
\end{itemize}


For example, scannerless parsing with an LR parsing algorithm 
will be able to parse this string. Therefore, one might argue
that the burden is on the parsing software to handle this issue.
The counterargument is that the language designer can avoid
this issue by being thoughtful of it during the design process
and verifying that these conflicts do not exist using proper
tools.

(Check to see whether lex's lookahead operator can solve this.
this would work if the length used for longest match rule includes
the lookahead string (seems unlikely but who knows))

Assuming you are using traditional scanning software,
a strategy for handling longest match conflicts, 
which makes the grammar less portable between various parsing tools.

\subsection{Context-Invasive Lexical Conflicts}
A scanner conflict that only occurs because a
token is active that is not associated with the
current parsing context is given the name
\newterm{context-invasive conflict}{Context-invasive Token Conflict}
in this book.

\subsubsection{An Example of an Invasive Conflict}


\subsection{Resolving Lexical Conflicts}
\subsubsection{Lexical Modes}
Lexical modes are a tool for resolving lexical conflicts
by creating separate sets of active tokens.

\url{https://github.com/antlr/antlr4/blob/master/doc/lexer-rules.md#lexical-modes}

\subsubsection{Lexical Lookahead Operator}
A lexical lookahead operator can specify a required
pattern to follow a lexeme in order to match the token.
This pattern is not included in the lexeme.


\subsubsection{Lexical Predicates}
A lexical predicate is arbitrary code that runs in order
to determine whether to match the token.

\subsubsection{Match Multiple Tokens}

\subsection{Academic History of Token Conflicts}

\subsubsection{Nawrocki}
Token conflicts were first formalized by Nawrocki in 1991.

\subsubsection{Keynes}

The thick dot in some of his definitions are used two
different ways:\\
universal quantified predicate (text p. 5, pdf p. 11)\\
\url{http://www.open-std.org/jtc1/sc22/open/n3187.pdf}\\

set comprehension term p. 20\\
\url{https://www.cs.ox.ac.uk/files/3389/PRG68.pdf}\\

\subsubsection{Denny}




% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
% CHAPTER: Parsing Algorithms
%
\chapter{Parsing Algorithms}

\section{Left-to-Right Parsing}

\section{Left and Right Derivations}

\section{Top-down Parsing}
This produces a left-most derivation.
Top down mimics walking hierarchical data.

\subsection{Predicitive Parsing}
\url{https://www.tutorialspoint.com/compiler_design/compiler_design_top_down_parser.htm}

\section{Bottom-up Parsing}
This produces a right-most derivation.
But LR might be able to produce LL (top-down) output: 
\url{https://cs.stackexchange.com/a/69996}

\section{Parsing Families}
"The primary difference between how LL and LR parsers operate is that an LL parser outputs a pre-order traversal of the parse tree and an LR parser outputs a post-order traversal."\\
\url{http://blog.reverberate.org/2013/07/ll-and-lr-parsing-demystified.html}

\vspace{10pt}

\url{http://blog.reverberate.org/2013/09/ll-and-lr-in-context-why-parsing-tools.html}

\vspace{10pt}

\url{https://stackoverflow.com/questions/5975741/what-is-the-difference-between-ll-and-lr-parsing}

\vspace{10pt}

\url{https://web.stanford.edu/class/archive/cs/cs143/cs143.1128/handouts/100%20Bottom-Up%20Parsing.pdf}

\vspace{10pt}

On why OCYacc generates a full LR(1) state machine:
"The reason for this is that, in todays' computing environment, it may not necessarily be desirable to reduce
the number of states in order to save space in the parsing tables."\\
\url{https://chaosinmotiondotblog.files.wordpress.com/2017/08/ocyacc-building-lr1-glr.pdf}


\section{The LL Parsing Family}
\section{The LR Parsing Family}
LR is particularly difficult to implement manually.

\section{CYK Parsing}

\section{Early Parsing}

\section{PEG Parsing}
Parser Expression Grammars (PEGs) are a bit of a mix
    Using syntactic predicates can preclude using an lr or ll parser because of unbounded lookahead

\section{Parser Combinators}
"Efficient Parsing with parsing combinators" J Kurs et al.


% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
% CHAPTER: LL Parsing
%
\chapter{LL Parsing}
\section{LL Table Structure}
\section{LL Table Building}

\section{LL(1) Parsing}

\section{LL(k) Parsing}

\section{GLL Parsing}
Afroozeh A., Izmaylova A. (2015) Faster, Practical GLL Parsing. In: Franke B. (eds) Compiler Construction. CC 2015. Lecture Notes in Computer Science, vol 9031. Springer, Berlin, Heidelberg

\section{LL(*) Parsing}
The parsing algorithm of ANTLR3.

\section{ALL(*) Parsing}
The parsing algorithm of ANTLR4.
\url{https://www.antlr.org/papers/allstar-techreport.pdf}


% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
% CHAPTER: LR Parsing
%
\chapter{LR Parsing}


\section{LR Table Structure}
\section{LR Table Building}

\subsection{LR Table Conflicts}
\subsection{Avoiding LR Table Conflicts}
\subsection{Resolving LR Table Conflicts}

\section{SLR(0) Parsing}

\section{Canonical LR(1) Parsing}
Uncompressed, not large. "Large" is a carryover from a time
when an LR table challenged memory resources of a 
typical computer. The difference now, between 100K and 1M
is not much. In the 1980's, that could have meant much more.

On why OCYacc generates a full LR(1) state machine:
"The reason for this is that, in todays' computing environment, it may not necessarily be desirable to reduce
the number of states in order to save space in the parsing tables."\\
\url{https://chaosinmotiondotblog.files.wordpress.com/2017/08/ocyacc-building-lr1-glr.pdf}

\section{LALR Parsing}
LALR is an optimization for reducing the size of the Canonical LR
tables by having certain similar states share the entry in the
parsing tables.
However, some languages that are correctly parsed by CLR
are not parsed correctly with the smaller LALR table. Therefore,
LALR is considered less powerful than CLR. 

\section{Minimal LR Parsing}
It has been shown that there is a relatively simple
method to determine which set of LR(1) states might cause 
a conflict if they share the same table entry.

A \newterm{Minimal LR Parser}{Minimal LR Parser}
only merges the states that are safe to merge.  
If the language requires separate states that
LR(1) supports then those states will be left unmerged, but 
if the language is simpler and merging states does not create
a conflict, then the states are merged.
In this way, the parsing table is roughly sized according to the needs of
the language.

This begs the question, why hasn't Minimal LR parsing been
more popular?

\url{https://stackoverflow.com/a/42915777}

\section{GLR Parsing}
Ambiguity can be a PITA. Multiple parse trees are difficult to deal with.
Tree-sitter uses GLR.

% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
% CHAPTER: Indentation-Sensitive Parsing
%
\chapter{Indentation-Sensitive Parsing}

\section{Indentation-Sensitive Languages}

Indented Languages are not described by Context-Free Grammars

Python, YAML, and Haskell are indentation sensitive.

\url{http://trevorjim.com/parsing-not-solved/}
\url{http://trevorjim.com/python-is-not-context-free/}

\section{Parsing Indented Blocks}
\subsection{Grammar Annotations}

% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
\subsubsection{Grammar terms annotated with Counts}
Indentation Sensitive Languages\\
2006 Leonhard Brunauer, Bernhard Muhlbacher\\
\url{https://www.google.com/search?q=LeonhardBrunauer+and+Bernhard+M%C3%BChlbacher.+Indentation+sensitive+languages}

The scheme allows a specific character to be matched a specified number of times.

For example, a tab character, represented by $\rightarrow$, can be repeated $m$ 
times in a grammar with this syntax:\\
$C_\rightarrow^m$

This rule states the start variable expands to zero tabs followed by a statement:\\
$S\rightarrow C_\rightarrow^0 Statement$

This rule states that the statement after an
"if" condition has greater indentation:\\
$C_\rightarrow^m Statement \rightarrow C_\rightarrow^m \: "if" \: Condition \: ":" \: newline \: C_\rightarrow^{m+1} Statement$

\begin{verbatim}
if true:
    a=1
\end{verbatim}


\subsubsection{Relationship Annotated BNF}
Principled Parsing for Indentation-Sensitive Languages 2013\\
\url{https://michaeldadams.org/papers/layout_parsing/LayoutParsing.pdf}



\subsection{Explicit Indent Checking?}

\subsubsection{GLR Filtering}
Layout-Sensitive Generalized Parsing\\
Erdweg S., Rendel T., Kästner C., Ostermann K. (2013) Layout-Sensitive Generalized Parsing. In: Czarnecki K., Hedin G. (eds) Software Language Engineering. SLE 2012. Lecture Notes in Computer Science, vol 7745. Springer, Berlin, Heidelberg

\subsubsection{Data-Dependent Grammars}
Ch. 3: "In this chapter we propose a parsing framework that embraces context information in its core."\\
\url{https://homepages.cwi.nl/~jurgenv/papers/PhDThesis_Ali_Afroozeh_and_Anastasia_Izmaylova.pdf}\\

\vspace{10pt}

Section 3.1:
\par Decls ::= \textbf{align} (\textbf{offside} Decl)*
\par \:\:\:\: $\vert$ \textbf{ignore}('\{' Decl(';' Decl)* '\}')

This definition clearly and concisely specifies that either all declarations in the list are
aligned, and each Decl is offsided with regard to its first token (first alternative), or
indentation is ignored inside curly braces (second alternative).

align and offside keywords desugared to data-dependent expressions (section 3.3.5)

\subsection{PEG Semantic Predicates}
\url{https://gist.github.com/dmajda/04002578dd41ae8190fc}\\
Note that the 'I' provides an explicit token indicating that
indentation is coming so this doesn't really work
for arbitrary indentation like YAML.

\subsection{Pseudo-Tokens and Pseudo-Grammars}

\subsubsection{Indented Blocks}
INDENT means match pending spacing more than last indent
and increases the indent level by one.
\begin{verbatim}
indents.push(new_indent))
\end{verbatim}

DEDENT means pending spacing less than last indent.
DEDENT represents the reduction of one 
level of current indentation. Multiple DEDENTs
may appear in sequence.
\begin{verbatim}
indents.pop()
\end{verbatim}
*consumes no input*

Parsing indentation requires context aware lexing.
Otherwise, a dedent would always be active.
Also, verbatim text would need a way to actively turn
off the indentation tokenization somehow.

\subsubsection{Explicit Current Indentation}
SAMEDENT means match pending spaces equal to last indent.\\
Prior Work:\\
\url{https://stackoverflow.com/questions/11659095/parse-indentation-level-with-peg-js}

Implementation strategy:\\
If you use SAMEDENT and the parser has an adaptive lexer, 
you can limit indentation logic to just the tokenizing
of specific tokens (INDENT/SAMEDENT/DEDENT) only when they
are active. If you wanted to combine a grammar with
indent tokens with a grammar without indent tokens
(as a sublanguage for example), then it makes the lexer
easier if it knows, explicitly, when indentation is relevant,
which makes it easier to write.

\subsubsection{Implicit Current Indentation}
Same indentation is implied in the grammar, 
I.E. indent checking is always on.
\url{https://docs.python.org/3/reference/grammar.html}
\begin{verbatim}
suite: simple_stmt | NEWLINE INDENT stmt+ DEDENT
\end{verbatim}
note that stmt does not refer to indentation (no SAMEDENT)

% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
If you can't use SAMEDENT because the tokenizor cannot (or it's too difficult to)
keep enough context, then indentation logic will need to be pervasive across the
tokenizer, activating after every newline. if you need to suspend indentation, 
you'll need to program that into the tokenizer for that situation (opt-out)

\section{Parsing Flow-Style Lists}

\subsection{Grammar}
First field needs to be defined differently (without samedent) than remaining fields
\begin{verbatim}
MARK_INDENT means indents.push(cur col), consume no input
\end{verbatim}
Use MARK\_INDENT after LIST\_SEPARATOR

\section{Parsing Blank Lines in Indented Blocks}
We need to swap tokens. Adding psuedo tokens is not helpful.
Stream based token swapping solves this.



% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
% CHAPTER: Constructing a Parser
%
\chapter{Constructing a Parser}
You can hand write a parser or use a parser generator to automatically
generate a parser
PEG is typically partially handwritten and partially generated.

\begin{itemize}
    \item Prototype with custom tokenizer
    \item LR(1) Grammar with no token conflicts
    \item Validate with various Parser Generators
    \item Rewrite as top down
    \item Fuzz test for both, validating equivalency
\end{itemize}


\section{Handwritten Parsers}

\section{Parser Generators}
Reality of Parser Generators
\begin{itemize}
    \item 
    Limited language support
        Not necessarily an issue if you are just trying to prove out your grammar
    \item 
    Multiple approaches to grammar syntax and expressibility
    \item 
    No standard lexing strategy
    \item 
    No standard parse trees or eventing (sax)
    \item 
    No standard code integration strategy with the grammar (see PEG.js example for indents)
    \item 
    So many to evaluate
\end{itemize}

Selected Parser Generators
\begin{itemize}
    \item LALR Parsing
    \begin{itemize}
        \item Flex/Bison
        \item Jison
    \end{itemize}
    \item ALL Parsing
    \begin{itemize}
        \item ANTLR4
    \end{itemize}
    \item GLR
    \begin{itemize}
        \item Tree-Sitter
    \end{itemize}
    \item PEG
    \begin{itemize}
        \item PEG.js
    \end{itemize}
\end{itemize}




% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
% CHAPTER: A Model Data Parser
%
\chapter{A Model Data Parser}

\section{Design Choices}
\begin{itemize}
\item
Grammar driven dynamic Lexing
\item
Token-specific custom lexing
\item
Emit multiple tokens per scan
\item
Minimal LR(1) Parsing
\item
Layered Grammar
\begin{itemize}
    \item map: map-text
    \item list: list-text
\end{itemize}
\end{itemize}


\appendix

% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
%
% Appendix: Data
%
\chapter{Data}\label{ch.Data}

\section{Motivation}
This book covers the topic of data because a parser
operates on data, and there are important issues to be
aware of with a common form of data called Unicode.
These issues are listed at the beginning of this chapter
in order to motivate the reader to explore these topics
further in detail.
The rest of the chapter is written to give the reader a
full understanding of these issues, from the ground up,
so that the reader can be confident that they have dealt
with them appropriately when they arise in the context of
parsing or any other context.

The first issue is that there are some symbols in the Unicode standard
which have multiple definitions. This can lead to potential
problems if this issue is not accounted for properly. For example,
it requires a strategy for determining whether two names, for
example, are the same. Another set of issues arise because it may
be unclear what one means by the \textit{length} of a sequences
of symbols.

These issues with Unicode are typically either not well known or not 
understood by both the users and developers of parsing software.
When the strategy for handling these questions is left undocumented
there is risk that software will behave in an unexpected way, 
potentially leading to serious consequences.

\section{Symbols}
% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
The signs, sounds, marks, and words humans use to represent ideas are called
\newterm{symbols}{Symbol}. For example, the twenty six letters of
the English alphabet are symbols representing sounds or parts of words. Words
are also symbols for the ideas they represent. 
The numerals 0 through 9 are symbols for numbers.
As long as something represents, or \textit{symbolizes} something else, 
it is a symbol. We use symbols as sort of "real world" currency
to remember or communicate something meaningful to ourselves or others.

\subsection{Interpreting Symbols}
Wikipedia defines
\newterm{data}{Data} as any sequence of symbols for which "a specific act of
interpretation" gives those symbols
meaning\footnote{\url{https://en.wikipedia.org/wiki/Data\_(computing)}}.
In other words, the essential nature of data is that is in a
\newterm{sequence}{Sequence}, which means that the symbols occur one after
another, and that there is a particular meaning associated with 
the ordering and choice of the symbols in the data.

\subsection{Manipulating Symbols}

% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
Computers are well known for their ability to manipulate data.
A computer can read and \newterm{execute}{Execution}, or follow,
instructions as represented by a sequence of symbols.
These instructions can, in
turn, direct the manipulation of symbols stored in the computer's memory.
It would be confusing to think of a computer working with the kind of visual
symbols that humans typically think of like a + (plus) sign or the numeral 2,
because they do not actually work directly with those kinds of symbols.
The symbols that a computer can read, follow, and manipulate are quite limited.
In fact at the lowest levels, there are only two symbols. Inside the computer,
these symbols exist as electrical states inside the transistors and wires of 
the computer. In an electric model, we think of these states
as  \textit{off} and \textit{on} and by convention these states symbolize the
numbers 0 and 1, respectively.

\section{Binary Data}\label{bindata}
\subsection{Bits}
A numerical symbol which is either 0 or 1 is known as a
\newterm{binary digit}{Binary Digit}, or \newterm{bit}{Bit} for short.
At the most fundamental level, bits are the letters of a very simple
alphabet that both humans and computers can manipulate in an
equivalent way. For example, both humans and computers are capable 
of numerically adding together the symbols 1 and 0 to produce the 
symbol 1, although they go about it in different ways. It is 
through these common definitions and ways of processing
symbols that computers can perform computations
that are relevant to humans.

% 0000000100000000020000000003000000000400000000050000000006000000000700000000008
\subsection{Binary Codes}
Since computers are only able to manipulate sequences of bits, we
have devised an incredibly rich variety of ways, known as 
\newterm{codes}{Codes}\footnote{\url{https://en.wikipedia.org/wiki/Code}} to map
the information available to us into sequences of ones and zeros. For example,
\newterm{decimal}{Decimal}\footnote{\url{https://en.wikipedia.org/wiki/Decimal}} 
numbers, which are the standard numbers we use from day to day, can be converted
to a \newterm{binary number}{Binary Number} composed of binary digits, using a
\newterm{binary code}{Binary Code} as seen in Table 1.1.


\begin{table}[h!]
\centering
\begin{tabular}{||c|c||}
    \hline
    Decimal & Binary\\
    \hline\hline
     0 & 000 \\
     \hline
     1 & 001 \\
     \hline
     2 & 010 \\
     \hline
     3 & 011 \\
     \hline
     4 & 100 \\
     \hline
     5 & 101 \\
     \hline
     6 & 110 \\
     \hline
     7 & 111 \\
     \hline
\end{tabular}
\caption{Map of Decimal Digits to Binary Digits}
\end{table}

\subsection{Bytes and Octets}
A \newterm{byte}{Byte} is typically 8 bits. A byte also
represents the smallest unit of a computer's memory that
is \textit{addressable} by location. Computer systems store and 
retrieve bytes of data in 
specific locations of the computer's memory and these locations are 
identified using a number, called a
\newterm{memory address}{Memory Address}.
There are
examples of old computers that had more or less bits but it is
standard now to assume a byte has 8 bits. In some contexts, such
as with networking standards, the term \newterm{octet}{Octet} may be used
instead of byte if there is a concern that the definition of a byte
is ambiguous.

\subsection{Base-N Number Systems}
People typically communicate using numbers which are split into digits
based on the number ten. This is called the 
\newterm{decimal numeral system}{Decimal Numeral System} 
When a digit exceeds nine, another digit to the left represents ten
times the current digit. Further digits can be added to represent 
higher numbers based on powers of ten. Numbers in the decimal number 
system are also called \newterm{base-10}{Base-10} numbers.

With binary numbers however, when a digit exceeds \textit{one},
another digit is added to the left that represents \textit{twice}
the current digit. Binary numbers
are based on powers of two and are called \newterm{base-2}{Base-2} numbers.

In addition to these number systems, two other systems are commonly
used with computer systems, called \newterm{hexadecimal}{Hexadecimal} 
and \newterm{octal}{Octal} numbers which
are base-16 and base-8 numbers respectively. 

\subsection{Octal Notation}
Octal is a numerical encoding that allows the use of
8 different symbols for each digit, 0 through 7.
Each symbol encodes 3 bits of a binary number.


\subsection{Hexadecimal Notation}
Hexadecimal is a numerical encoding that allows the use of
16 different symbols for each digit, 0 through 9 and A through F. Lowercase
letters are also used but mixing upper and lowercase letters is uncommon.
Each symbol encodes 4 bits, or a nibble, of a binary number.
Hexadecimal numbers are
convenient because you can always represent one byte with only two digits.


\begin{table}[h!]
\centering
\begin{tabular}{||c|c|c|c|c|c||}
    \hline
    Decimal & Binary & Octal & Hexadecimal\\
    \hline\hline
     0 & 0 & 0 & 0\\
     \hline
     1 & 1 & 1 & 1\\
     \hline
     2 & 10 & 2 & 2\\
     \hline
     3 & 11 & 3 & 3\\
     \hline
     4 & 100 & 4 & 4\\
     \hline
     5 & 101 & 5 & 5\\
     \hline
     6 & 110 & 6 & 6\\
     \hline
     7 & 111 & 7 & 7\\
     \hline
     8 & 1000 & 10 & 8\\
     \hline
     9 & 1001 & 11 & 9\\
     \hline
     10 & 1010 & 12 & A\\
     \hline
     11 & 1011 & 13 & B\\
     \hline
     12 & 1100 & 14 & C\\
     \hline
     13 & 1101 & 15 & D\\
     \hline
     14 & 1110 & 16 & E\\
     \hline
     15 & 1111 & 17 & F\\
     \hline
     16 & 10000 & 18 & 10\\
     \hline
     17 & 10001 & 19 & 11\\
     \hline
     18 & 10010 & 20 & 12\\
     \hline
     19 & 10011 & 21 & 13\\
     \hline
\end{tabular}
\caption{Numbers in Various Numeric Notations}
\end{table}



% 0000000100000000020000000003000000000400000000050000000006000000000700000000008

\section{Character Encodings}
Characters are symbols that represent sounds or parts of words.

\subsection{ASCII}
An early standard character encoding was ASCII, which stands for
American Standard Code for Information Interchange.

\subsection{Terminal Control Characters}
A computer terminal is composed of a display monitor for displaying
characters to humans and a keyboard to send characters to the
computer. These characters are displayed in rows and columns.

Control characters were invented in order to control terminal 
operations using character codes, such as sending a character code
to go to the next line. Character codes are still supported, although
many of them are outdated.

\subsection{Unicode and UTF-8}
The Unicode\textcircled{r} Standard is a universal character
encoding standard for all written characters and text.

\subsubsection{Codepoints}

\subsubsection{Graphemes}


\subsubsection{Unicode Normalization}
Unicode Normalization is a process used to resolve ambiguities in the
Unicode Standard.

\section{Summary of Chapter \ref{ch.Data}}

\backmatter
\addcontentsline{toc}{chapter}{Index}
\printindex
\end{document}

